<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="yanm1ng&#39;s blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      Hierarchical Question-Image Co-Attention for Visual Question Answering | Leslie
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script>
  
  
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


  
  
    
<script src="/js/local-search.js"></script>


<meta name="generator" content="Hexo 7.0.0"></head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>
  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Leslie</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/series/" class="item-link">Series</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
      
        <li class="menu-item menu-item-search right-list">
    <a role="button" class="popup-trigger">
        <i class="fa fa-search fa-fw"></i>
    </a>
</li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/series/" class="menu-link">Series</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
    
      <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
            <span class="search-icon">
                <i class="fa fa-search"></i>
            </span>
            <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off"
                    placeholder="Please enter your keyword(s) to search." spellcheck="false"
                    type="search" class="search-input">
            </div>
            <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>
    
  </div>
</header>

    <div id="article-banner">
  <h2>Hierarchical Question-Image Co-Attention for Visual Question Answering</h2>
  <p class="post-date">2021-05-18</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p style="text-indent:2em">
这篇文章提出了“协同注意力”机制，不只考虑视觉上的注意力（看哪儿），同时也考虑问题本身（重点词）。
另外，在图像和问题解析时构建“单词-短语-问题”三层分层结构。单词层使用嵌入矩阵将单词转化成词向量；
短语层使用1维核卷积获取词组中包含的信息，通过将词表征向量和不同时序过滤器做卷积，然后将多个
n-元词组通过池化得到一个短语层的表征向量；在问题层，使用循环神经网络编码整个问题。
</p>

<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p style="text-indent:2em">
</p>

<h2 id="图注意力"><a href="#图注意力" class="headerlink" title="图注意力"></a>图注意力</h2><ul>
<li>spatial attention<br>  在标准LSTM模型中添加空间注意力机制</li>
<li>compositional scheme<br>  由文本解析器和多个神经网络模块组成，由解析器决定调用哪个模块解答问题，模块化网络参考论文<br>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.02799">Deep Compositional Question Answering with Neural Module Networks</a></li>
<li>stacked attention network<br>  多跳图像注意力框架;先将单词映射到图像区域，再通过注意力机制选择相关区域形成解答</li>
<li>dynamic memory<br>  使用新的输入融合模型和基于GRU的注意力机制得到答案</li>
</ul>
<h2 id="语言注意力"><a href="#语言注意力" class="headerlink" title="语言注意力"></a>语言注意力</h2><p style="text-indent:2em">
在VQA领域还没有相关的工作，不过在NLP领域对此有较多研究。
</p>

<ul>
<li>Neural machine translation by jointly learning to align and translate</li>
<li>Reasoning about entailment with neural attention</li>
<li>Teaching machines to read and comprehend</li>
<li>Abcnn: Attention-based convolutional neural network for modeling sentence pairs</li>
<li>Attentive pooling networks</li>
</ul>
<h1 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h1><h2 id="符号介绍"><a href="#符号介绍" class="headerlink" title="符号介绍"></a>符号介绍</h2><p>包含 $T$ 个字符的问题的向量表示记为 $Q&#x3D;\lbrace q_{1}, … , q_{t} \rbrace$, $q_{t}$ 表示第 $t$ 个词的特征向量。<br>$q_{t}^{w}$,$q_{t}^{p}$,$q_{t}^{s}$分别表示 $t$ 位置的词嵌入，短语嵌入和问题嵌入向量。图像特征记为<br>$V &#x3D; \lbrace \nu_{1}, …, \nu_{N} \rbrace$,$\nu_{n}$是空间位置$n$的特征向量。每层的图像和问题的协同注意力特征<br>记为$\hat{\nu}^{r}$和$\hat{q}^{r}$, $r \in \lbrace w, p, s \rbrace$。</p>
<h2 id="问题分层"><a href="#问题分层" class="headerlink" title="问题分层"></a>问题分层</h2><p>将问题中的单词用$1-hot$编码表示，先将单词编码映射到单词层向量$Q^{w}$。短语层的向量使用下式表示：<br>$$\hat{q}<em>{s,t}^{p} &#x3D; tanh(W</em>{c}^{s}q_{t:t+s-1}^{w}), s \in \lbrace 1, 2, 3 \rbrace$$<br>其中$W_{c}^{s}$为权重参数，$t$位置的短语层特征取<br>$$q_{t}^{p} &#x3D; max(\hat{q}<em>{1,t}^{p}, \hat{q}</em>{2,t}^{p}, \hat{q}_{3,t}^{p}), t \in \lbrace 1, 2, …, T \rbrace$$</p>
<h2 id="协同注意力机制"><a href="#协同注意力机制" class="headerlink" title="协同注意力机制"></a>协同注意力机制</h2><p>文章提供了两种生成图像和问题映射的协同注意力机制。</p>
<div>
<img src="https://d3i71xaburhd42.cloudfront.net/fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b/250px/4-Figure2-1.png" width="70%" />
Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism. 
</div>

<h3 id="Parallel-Co-Attention"><a href="#Parallel-Co-Attention" class="headerlink" title="Parallel Co-Attention"></a>Parallel Co-Attention</h3><p>Parallel Co-Attention通过计算image和question特征之间的相似性，使image和question联系起来，相关矩阵的计算方法：<br>$$C &#x3D; tanh(Q^{T}W_{b}V)$$<br>把$C$当成一种特征，预测image和question的attention maps。</p>
<p>$$H^{v} &#x3D; tanh(W_{v}+(W_{q}Q)C), H^{q} &#x3D; tanh(W_{q}Q + (W_{v}V)C^{T})$$<br>$$a^{\nu} &#x3D; softmax(\omega_{hv}^{T}H^{\nu}), a^{q} &#x3D; softmax(\omega_{hq}^{T}H^{q})$$<br>$$\hat{v} &#x3D; \sum_{n&#x3D;1}^{N}{a_{n}^{v}v_{n}}, \hat{q} &#x3D; \sum_{t&#x3D;1}^{T}{a_{t}^{q}v_{t}}$$</p>
<h3 id="Alternating-Co-Attention"><a href="#Alternating-Co-Attention" class="headerlink" title="Alternating Co-Attention"></a>Alternating Co-Attention</h3><p>整个过程分为三部分：</p>
<ol>
<li>将问题映射成一个单向量$q$</li>
<li>基于$q$，集中注意于image</li>
<li>基于attended image feature, 集中注意question</li>
</ol>
<p>注意力的操作部署如下：<br>$$H &#x3D; tanh(W_{x}X + (W_{g}g)\mathbb{1}^{T})$$<br>$$a^{x} &#x3D; softmax(w_{hx}^{T}H)$$<br>$$\hat{x} &#x3D; \suma_{i}^{x}x_{i}$$</p>
<p>第一步： X &#x3D; Q, and g is 0;<br>第二步： X &#x3D; V where V is the image features，guidance g is intermediate attended question feature ^s from the first step<br>第三步： we use the attended image feature ^v as the guidance to attend the question again, i.e., X &#x3D; Q and g &#x3D; ^v.</p>
<h3 id="预测答案编码"><a href="#预测答案编码" class="headerlink" title="预测答案编码"></a>预测答案编码</h3><p>文章将VQA视为分类任务，基于图像和问题的协同注意力特征预测答案。</p>
<div>
<img src="https://d3i71xaburhd42.cloudfront.net/fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b/5-Figure3-1.png" width="70%" />
Figure 3: (a) Hierarchical question encoding; (b) Encoding for predicting answers
</div>

<p>按上图右所示，使用MLP循环编码注意力特征得到最终答案。</p>
<p>$$h^{w} &#x3D; tanh(W_{w}(\hat{q}^{w} + \hat{v}^{w}))$$<br>$$h^{p} &#x3D; tanh(W_{p}[(\hat{q}^{p} + \hat{v}^{p}), h^{w}])$$<br>$$h^{s} &#x3D; tanh(W_{s}[(\hat{q}^{s} + \hat{v}^{w}), h^{p}])$$<br>$$p &#x3D; softmax(W_{h}h^{s})$$</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#VQA" >
    <span class="tag-code">VQA</span>
  </a>

  <a href="/tags#Attention" >
    <span class="tag-code">Attention</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2021/05/14/%E8%B4%BA%E7%8E%9F%E8%8F%81%E7%9A%84%E6%96%B0%E8%A2%AB%E5%AD%90/">
        <span class="nav-arrow">← </span>
        
          贺玟菁的新被子
        
      </a>
    
    
      <a class="nav-right" href="/2021/05/20/R%E8%AF%AD%E8%A8%80%E5%B0%8F%E6%8A%80%E5%B7%A7/">
        
          R语言小技巧
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
    <!-- 二维码 END -->
    
      <!-- Utterances START -->
      <div id="utterances"></div>
      <script src="https://utteranc.es/client.js"
        repo=""
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async></script>    
      <!-- Utterances END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-nav-text">简介</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-nav-text">相关工作</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-nav-text">图注意力</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E8%AF%AD%E8%A8%80%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-nav-text">语言注意力</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95"><span class="toc-nav-text">研究方法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%AC%A6%E5%8F%B7%E4%BB%8B%E7%BB%8D"><span class="toc-nav-text">符号介绍</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E9%97%AE%E9%A2%98%E5%88%86%E5%B1%82"><span class="toc-nav-text">问题分层</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%8D%8F%E5%90%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-nav-text">协同注意力机制</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Parallel-Co-Attention"><span class="toc-nav-text">Parallel Co-Attention</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Alternating-Co-Attention"><span class="toc-nav-text">Alternating Co-Attention</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%A2%84%E6%B5%8B%E7%AD%94%E6%A1%88%E7%BC%96%E7%A0%81"><span class="toc-nav-text">预测答案编码</span></a></li></ol></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://leslie-arch.github.io/2021/05/18/Hierarchical-Question-Image-Co-Attention-for-Visual-Question-Answering/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', '/css/images/error_icon.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== '/css/images/error_icon.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>







    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2024 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a target="_blank" rel="noopener" href="https://github.com/yanm1ng">yanm1ng</a>
    
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      hljs.configure({useBR: true});
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->


<script src="/js/script.js"></script>


  </body>
</html>